{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e807dfc",
   "metadata": {},
   "source": [
    "# Step : Review Collection - Tutorial\n",
    "\n",
    "**Purpose:** Collect Google Maps reviews for discovered business locations\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to use the ReviewCollector\n",
    "- Different output modes (CSV, JSON, JSON-per-city, JSON-per-business)\n",
    "- Checkpoint and resume functionality\n",
    "- Review filtering and validation\n",
    "\n",
    "**For Junior Developers:**\n",
    "- Tests progress from simple to complex\n",
    "- Clear outputs show what data looks like\n",
    "- Checkpoint system prevents data loss on interruption\n",
    "- Multiple output formats for different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748bbb6",
   "metadata": {},
   "source": [
    "## What's New: Enhanced Place ID Support\n",
    "\n",
    "This notebook now supports the **enhanced discovery system** with:\n",
    "- **Canonical Place IDs**: Validated `ChIJ...` format for reliable review collection\n",
    "- **Multiple ID types**: Handles `canonical_place_id`, `place_id`, and `data_id`\n",
    "- **Backward compatibility**: Works with both old and new discovery formats\n",
    "\n",
    "**New columns you might see:**\n",
    "- `canonical_place_id`: Resolved canonical Google Maps place ID\n",
    "- `resolve_status`: How the ID was obtained (cache/api_resolved/already_canonical)\n",
    "- `data_id`: Numeric CID-like identifier (fallback if canonical not available)\n",
    "\n",
    "The collection system automatically uses the best available ID for each location!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa791c3",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5264949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our modules\n",
    "from review_analyzer.collect import ReviewCollector\n",
    "from review_analyzer import config\n",
    "\n",
    "print(\" All imports successful!\")\n",
    "print(f\"\\nAvailable output modes:\")\n",
    "print(f\" - {config.OutputMode.CSV}\")\n",
    "print(f\" - {config.OutputMode.SINGLE_JSON}\")\n",
    "print(f\" - {config.OutputMode.JSON_PER_CITY}\")\n",
    "print(f\" - {config.OutputMode.JSON_PER_BUSINESS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f205d",
   "metadata": {},
   "source": [
    "## Data Architecture Overview\n",
    "\n",
    "The pipeline uses an organized folder structure:\n",
    "\n",
    "```\n",
    "data/\n",
    " 00_config/ # Static configurations\n",
    " cities/ # City aliases, coordinates, regions.geojson\n",
    " templates/ # Business templates (banks_template.csv)\n",
    " 0_raw/ # Immutable source data\n",
    " discovery/ # Discovered places (timestamped folders)\n",
    " reviews/ # Raw reviews (timestamped folders)\n",
    " 0_interim/ # Recomputable cache\n",
    " collection/ # Reviews from collect step\n",
    " transform/ # Normalized reviews + regions\n",
    " checkpoints/ # Resume points for long operations\n",
    " 0_processed/ # Final outputs\n",
    " discovery/ # Processed agency lists\n",
    " collection/ # Collected reviews\n",
    " classification/ # Classified reviews (with sentiments/topics)\n",
    " 0_analysis/ # Reports, figures, dashboards\n",
    " 99_archive/ # Deprecated data\n",
    "\n",
    "logs/ # Pipeline execution logs\n",
    "```\n",
    "\n",
    "**This notebook saves data to:**\n",
    "- `data/0_interim/collection/` - Collected reviews (Parquet + CSV)\n",
    "- `data/0_processed/collection/` - Test outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a7d99",
   "metadata": {},
   "source": [
    "## Test : Initialize Collector & Check Input Data\n",
    "\n",
    "**What this does:** \n",
    "- Creates ReviewCollector instance\n",
    "- Loads agencies from Step (discover_placeids.ipynb)\n",
    "\n",
    "**Expected output:** Confirmation of agencies loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collector = ReviewCollector(debug=True)\n",
    "\n",
    "print(\" ReviewCollector initialized successfully!\")\n",
    "print(f\" Debug mode: {collector.debug}\")\n",
    "print(f\" Client ready: {collector.client is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for input data from Step (or use latest from processed/)\n",
    "# Try new data architecture first\n",
    "input_file = project_root / \"data\" / \"0_processed\" / \"discovery\" / \"agencies_discovered.csv\"\n",
    "\n",
    "# Fallback to legacy path\n",
    "if not input_file.exists():\n",
    " input_file = project_root / \"data\" / \"output\" / \"agencies_for_collection.csv\"\n",
    "\n",
    "if input_file.exists():\n",
    " agencies_df = pd.read_csv(input_file)\n",
    " \n",
    " print(f\" INPUT DATA LOADED\")\n",
    " print(f\"=\"*60)\n",
    " print(f\" File: {input_file.name}\")\n",
    " print(f\" Path: {input_file.parent}\")\n",
    " print(f\" Total agencies: {len(agencies_df)}\")\n",
    " print(f\" Columns: {list(agencies_df.columns)}\")\n",
    " \n",
    " # Check for new columns from enhanced discovery\n",
    " if 'canonical_place_id' in agencies_df.columns:\n",
    " print(f\"\\n Enhanced discovery data detected!\")\n",
    " canonical_count = agencies_df['canonical_place_id'].notna().sum()\n",
    " print(f\" Canonical place IDs: {canonical_count}/{len(agencies_df)}\")\n",
    " \n",
    " if 'resolve_status' in agencies_df.columns:\n",
    " print(f\"\\n Resolution status:\")\n",
    " status_counts = agencies_df['resolve_status'].value_counts()\n",
    " for status, count in status_counts.items():\n",
    " print(f\" - {status}: {count}\")\n",
    " \n",
    " # Support both old (_bank) and new (_business) column names\n",
    " business_col = '_business' if '_business' in agencies_df.columns else '_bank'\n",
    " if business_col in agencies_df.columns:\n",
    " print(f\"\\n Businesses: {agencies_df[business_col].nunique()}\")\n",
    " for business in agencies_df[business_col].unique()[:]: # Show first \n",
    " count = len(agencies_df[agencies_df[business_col] == business])\n",
    " print(f\" - {business}: {count} locations\")\n",
    " if agencies_df[business_col].nunique() > :\n",
    " print(f\" ... and {agencies_df[business_col].nunique() - } more\")\n",
    " \n",
    " if '_city' in agencies_df.columns:\n",
    " print(f\"\\n Cities: {agencies_df['_city'].nunique()}\")\n",
    " for city in agencies_df['_city'].unique()[:]: # Show first \n",
    " count = len(agencies_df[agencies_df['_city'] == city])\n",
    " print(f\" - {city}: {count} locations\")\n",
    " if agencies_df['_city'].nunique() > :\n",
    " print(f\" ... and {agencies_df['_city'].nunique() - } more\")\n",
    " \n",
    " print(f\"\\n Sample:\")\n",
    " display(agencies_df.head())\n",
    " \n",
    "else:\n",
    " print(\" No agencies file found!\")\n",
    " print(f\" Expected: {input_file}\")\n",
    " print(f\"\\n Please run discover_placeids.ipynb first to generate agencies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a3f86",
   "metadata": {},
   "source": [
    "## Test : Collect Reviews for Single Agency (CSV Mode)\n",
    "\n",
    "**What this does:** Collects reviews for ONE agency to test the system\n",
    "\n",
    "**Use case:** Quick test before full run\n",
    "\n",
    "**Expected output:** CSV file with reviews and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddfced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test file with single agency\n",
    "test_agencies = agencies_df.head().copy()\n",
    "test_input = project_root / \"data\" / \"0_interim\" / \"collection\" / \"test_single_agency.csv\"\n",
    "test_input.parent.mkdir(parents=True, exist_ok=True)\n",
    "test_agencies.to_csv(test_input, index=False)\n",
    "\n",
    "print(f\" TEST: Single Agency\")\n",
    "print(f\"=\"*60)\n",
    "print(f\" Agency: {test_agencies['title'].iloc[0] if 'title' in test_agencies.columns else test_agencies['_place_id'].iloc[0]}\")\n",
    "print(f\" Place ID: {test_agencies['_place_id'].iloc[0]}\")\n",
    "\n",
    "# Show additional info if available\n",
    "if 'canonical_place_id' in test_agencies.columns:\n",
    " canonical = test_agencies['canonical_place_id'].iloc[0]\n",
    " if pd.notna(canonical):\n",
    " print(f\" Canonical ID: {canonical}\")\n",
    "\n",
    "if 'resolve_status' in test_agencies.columns:\n",
    " status = test_agencies['resolve_status'].iloc[0]\n",
    " print(f\" Resolution: {status}\")\n",
    "\n",
    "# Output paths (use new data architecture)\n",
    "output_path = project_root / \"data\" / \"0_processed\" / \"collection\" / f\"test_single_reviews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_path = project_root / \"data\" / \"0_interim\" / \"collection\" / \"checkpoints\" / \"test_single_checkpoint.json\"\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n Output: {output_path.name}\")\n",
    "print(f\" Checkpoint: {checkpoint_path.name}\\n\")\n",
    "\n",
    "# Collect reviews\n",
    "stats = collector.collect_reviews(\n",
    " input_file=test_input,\n",
    " output_mode=\"csv\",\n",
    " output_path=output_path,\n",
    " checkpoint_file=checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" COLLECTION STATS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    " print(f\" {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca60837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect collected reviews\n",
    "if output_path.exists():\n",
    " reviews_df = pd.read_csv(output_path)\n",
    " \n",
    " print(f\"\\n REVIEWS COLLECTED\")\n",
    " print(f\"=\"*60)\n",
    " print(f\" Total reviews: {len(reviews_df)}\")\n",
    " print(f\" Columns: {list(reviews_df.columns)}\\n\")\n",
    " \n",
    " # Show sample reviews\n",
    " print(\"Sample reviews:\")\n",
    " display(reviews_df.head())\n",
    " \n",
    " # Rating distribution\n",
    " if 'rating' in reviews_df.columns:\n",
    " print(f\"\\n Rating Distribution:\")\n",
    " rating_counts = reviews_df['rating'].value_counts().sort_index()\n",
    " for rating, count in rating_counts.items():\n",
    " stars = '' * int(rating)\n",
    " bar = '' * (count * 0 // len(reviews_df))\n",
    " print(f\" {stars} ({rating}): {count:} {bar}\")\n",
    " \n",
    " # Review length statistics\n",
    " if 'text' in reviews_df.columns:\n",
    " reviews_df['text_length'] = reviews_df['text'].fillna('').str.len()\n",
    " print(f\"\\n Review Length Stats:\")\n",
    " print(f\" Average: {reviews_df['text_length'].mean():.0f} characters\")\n",
    " print(f\" Median: {reviews_df['text_length'].median():.0f} characters\")\n",
    " print(f\" Max: {reviews_df['text_length'].max():.0f} characters\")\n",
    "else:\n",
    " print(f\" Output file not found: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a34fc9",
   "metadata": {},
   "source": [
    "## Test : Collect for Multiple Agencies (CSV Mode)\n",
    "\n",
    "**What this does:** Collects reviews for agencies\n",
    "\n",
    "**Use case:** Medium-sized test run\n",
    "\n",
    "**Expected output:** Combined CSV with reviews from all agencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test file with agencies\n",
    "test_agencies = agencies_df.head().copy()\n",
    "test_input = project_root / \"data\" / \"0_interim\" / \"collection\" / \"test_five_agencies.csv\"\n",
    "test_input.parent.mkdir(parents=True, exist_ok=True)\n",
    "test_agencies.to_csv(test_input, index=False)\n",
    "\n",
    "print(f\" TEST: Five Agencies\")\n",
    "print(f\"=\"*60)\n",
    "print(f\" Agencies: {len(test_agencies)}\")\n",
    "if 'title' in test_agencies.columns:\n",
    " for idx, title in enumerate(test_agencies['title'], ):\n",
    " print(f\" {idx}. {title}\")\n",
    "\n",
    "# Output paths (use new data architecture)\n",
    "output_path = project_root / \"data\" / \"0_processed\" / \"collection\" / f\"test_five_reviews_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_path = project_root / \"data\" / \"0_interim\" / \"collection\" / \"checkpoints\" / \"test_five_checkpoint.json\"\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n Output: {output_path.name}\")\n",
    "print(f\" Checkpoint: {checkpoint_path.name}\\n\")\n",
    "\n",
    "# Collect reviews\n",
    "stats = collector.collect_reviews(\n",
    " input_file=test_input,\n",
    " output_mode=\"csv\",\n",
    " output_path=output_path,\n",
    " checkpoint_file=checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" COLLECTION STATS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    " print(f\" {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reviews by agency\n",
    "if output_path.exists():\n",
    " reviews_df = pd.read_csv(output_path)\n",
    " \n",
    " print(f\"\\n REVIEWS BY AGENCY\")\n",
    " print(f\"=\"*60)\n",
    " print(f\" Total reviews: {len(reviews_df)}\\n\")\n",
    " \n",
    " # Group by place_id\n",
    " if '_place_id' in reviews_df.columns:\n",
    " review_counts = reviews_df.groupby('_place_id').size().sort_values(ascending=False)\n",
    " print(\" Reviews per agency:\")\n",
    " for place_id, count in review_counts.items():\n",
    " # Get agency name if available\n",
    " agency_row = reviews_df[reviews_df['_place_id'] == place_id].iloc[0]\n",
    " name = agency_row.get('title', place_id[:0])\n",
    " print(f\" {name}: {count} reviews\")\n",
    " \n",
    " # Overall rating stats\n",
    " if 'rating' in reviews_df.columns:\n",
    " print(f\"\\n Overall stats:\")\n",
    " print(f\" Average rating: {reviews_df['rating'].mean():.f} \")\n",
    " print(f\" Median rating: {reviews_df['rating'].median():.f} \")\n",
    " print(f\" -star reviews: {(reviews_df['rating'] == ).sum()} ({(reviews_df['rating'] == ).sum()/len(reviews_df)*00:.f}%)\")\n",
    " print(f\" -star reviews: {(reviews_df['rating'] == ).sum()} ({(reviews_df['rating'] == ).sum()/len(reviews_df)*00:.f}%)\")\n",
    " \n",
    " # Sample\n",
    " print(f\"\\n Sample reviews:\")\n",
    " display(reviews_df.head())\n",
    "else:\n",
    " print(f\" Output file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382504b",
   "metadata": {},
   "source": [
    "## Test : JSON-per-City Mode\n",
    "\n",
    "**What this does:** Collects reviews and saves separate JSON file per city\n",
    "\n",
    "**Use case:** City-level analysis or reporting\n",
    "\n",
    "**Expected output:** One JSON file per city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use agencies from multiple cities if available\n",
    "test_agencies = agencies_df.head(0).copy()\n",
    "test_input = project_root / \"data\" / \"0_interim\" / \"collection\" / \"test_json_per_city_agencies.csv\"\n",
    "test_input.parent.mkdir(parents=True, exist_ok=True)\n",
    "test_agencies.to_csv(test_input, index=False)\n",
    "\n",
    "print(f\" TEST: JSON-per-City Mode\")\n",
    "print(f\"=\"*60)\n",
    "print(f\" Agencies: {len(test_agencies)}\")\n",
    "if '_city' in test_agencies.columns:\n",
    " cities = test_agencies['_city'].unique()\n",
    " print(f\" Cities: {len(cities)} ({', '.join(cities)})\")\n",
    "\n",
    "# Output directory (use new data architecture)\n",
    "output_dir = project_root / \"data\" / \"0_processed\" / \"collection\" / f\"json_per_city_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "checkpoint_path = project_root / \"data\" / \"0_interim\" / \"collection\" / \"checkpoints\" / \"test_json_city_checkpoint.json\"\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n Output dir: {output_dir.name}\")\n",
    "print(f\" Checkpoint: {checkpoint_path.name}\\n\")\n",
    "\n",
    "# Collect reviews\n",
    "stats = collector.collect_reviews(\n",
    " input_file=test_input,\n",
    " output_mode=\"json-per-city\",\n",
    " output_dir=output_dir,\n",
    " checkpoint_file=checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" COLLECTION STATS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    " print(f\" {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect JSON files created\n",
    "json_files = sorted(output_dir.glob(\"*.json\"))\n",
    "\n",
    "print(f\"\\n JSON FILES CREATED\")\n",
    "print(f\"=\"*60)\n",
    "print(f\" Total files: {len(json_files)}\\n\")\n",
    "\n",
    "for json_file in json_files:\n",
    " with open(json_file) as f:\n",
    " data = json.load(f)\n",
    " \n",
    " # Count reviews in this file\n",
    " total_reviews = sum(len(reviews) for reviews in data.values())\n",
    " \n",
    " print(f\" {json_file.name}\")\n",
    " print(f\" Places: {len(data)}\")\n",
    " print(f\" Reviews: {total_reviews}\")\n",
    " \n",
    " # Show structure of first place\n",
    " if data:\n",
    " first_place_id = list(data.keys())[0]\n",
    " first_reviews = data[first_place_id]\n",
    " print(f\" Sample place: {first_place_id}\")\n",
    " print(f\" Sample reviews: {len(first_reviews)}\")\n",
    " if first_reviews:\n",
    " print(f\" Sample review keys: {list(first_reviews[0].keys())}\")\n",
    " print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbccfa9",
   "metadata": {},
   "source": [
    "## Test : JSON-per-Business Mode\n",
    "\n",
    "**What this does:** Collects reviews and saves separate JSON file per business\n",
    "\n",
    "**Use case:** Business-level analysis or reporting\n",
    "\n",
    "**Expected output:** One JSON file per business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use agencies from multiple businesses if available\n",
    "test_agencies = agencies_df.head(0).copy()\n",
    "test_input = project_root / \"data\" / \"0_interim\" / \"collection\" / \"test_json_per_business_agencies.csv\"\n",
    "test_input.parent.mkdir(parents=True, exist_ok=True)\n",
    "test_agencies.to_csv(test_input, index=False)\n",
    "\n",
    "print(f\" TEST: JSON-per-Business Mode\")\n",
    "print(f\"=\"*60)\n",
    "print(f\" Agencies: {len(test_agencies)}\")\n",
    "\n",
    "# Support both old (_bank) and new (_business) column names\n",
    "business_col = '_business' if '_business' in test_agencies.columns else '_bank'\n",
    "if business_col in test_agencies.columns:\n",
    " businesses = test_agencies[business_col].unique()\n",
    " print(f\" Businesses: {len(businesses)} ({', '.join(businesses)})\")\n",
    "\n",
    "# Output directory (use new data architecture)\n",
    "output_dir = project_root / \"data\" / \"0_processed\" / \"collection\" / f\"json_per_business_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "checkpoint_path = project_root / \"data\" / \"0_interim\" / \"collection\" / \"checkpoints\" / \"test_json_business_checkpoint.json\"\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n Output dir: {output_dir.name}\")\n",
    "print(f\" Checkpoint: {checkpoint_path.name}\\n\")\n",
    "\n",
    "# Collect reviews\n",
    "stats = collector.collect_reviews(\n",
    " input_file=test_input,\n",
    " output_mode=\"json-per-business\",\n",
    " output_dir=output_dir,\n",
    " checkpoint_file=checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" COLLECTION STATS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    " print(f\" {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect JSON files created\n",
    "json_files = sorted(output_dir.glob(\"*.json\"))\n",
    "\n",
    "print(f\"\\n JSON FILES CREATED\")\n",
    "print(f\"=\"*60)\n",
    "print(f\" Total files: {len(json_files)}\\n\")\n",
    "\n",
    "for json_file in json_files:\n",
    " with open(json_file) as f:\n",
    " data = json.load(f)\n",
    " \n",
    " # Count reviews\n",
    " total_reviews = sum(len(reviews) for reviews in data.values())\n",
    " \n",
    " print(f\" {json_file.name}\")\n",
    " print(f\" Places: {len(data)}\")\n",
    " print(f\" Reviews: {total_reviews}\")\n",
    " print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e1550",
   "metadata": {},
   "source": [
    "## Test 6: Checkpoint and Resume\n",
    "\n",
    "**What this does:** Tests checkpoint/resume functionality\n",
    "\n",
    "**Scenario:** Simulates interrupted collection that can be resumed\n",
    "\n",
    "**Expected output:** Second run resumes from where first run stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a62ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test with agencies\n",
    "test_agencies = agencies_df.head().copy()\n",
    "test_input = project_root / \"data\" / \"0_interim\" / \"collection\" / \"test_checkpoint_agencies.csv\"\n",
    "test_input.parent.mkdir(parents=True, exist_ok=True)\n",
    "test_agencies.to_csv(test_input, index=False)\n",
    "\n",
    "output_path = project_root / \"data\" / \"0_processed\" / \"collection\" / f\"test_checkpoint_reviews.csv\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_path = project_root / \"data\" / \"0_interim\" / \"collection\" / \"checkpoints\" / \"test_checkpoint_demo.json\"\n",
    "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Delete old checkpoint if exists\n",
    "if checkpoint_path.exists():\n",
    " checkpoint_path.unlink()\n",
    " print(\" Deleted old checkpoint\\n\")\n",
    "\n",
    "print(f\" TEST: Checkpoint and Resume\")\n",
    "print(f\"=\"*60)\n",
    "print(f\" Agencies: {len(test_agencies)}\")\n",
    "print(f\"\\n RUN : Initial collection\")\n",
    "\n",
    "# First run\n",
    "stats = collector.collect_reviews(\n",
    " input_file=test_input,\n",
    " output_mode=\"csv\",\n",
    " output_path=output_path,\n",
    " checkpoint_file=checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"\\n First run complete\")\n",
    "print(f\" Places processed: {stats['total_places']}\")\n",
    "print(f\" Checkpoint created: {checkpoint_path.exists()}\")\n",
    "\n",
    "# Check checkpoint contents\n",
    "if checkpoint_path.exists():\n",
    " with open(checkpoint_path) as f:\n",
    " checkpoint_data = json.load(f)\n",
    " print(f\" Checkpoint contains: {list(checkpoint_data.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second run - should use checkpoint\n",
    "print(f\"\\n RUN : Resume from checkpoint\")\n",
    "print(f\" (Should skip already processed places)\\n\")\n",
    "\n",
    "stats = collector.collect_reviews(\n",
    " input_file=test_input,\n",
    " output_mode=\"csv\",\n",
    " output_path=output_path,\n",
    " checkpoint_file=checkpoint_path\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Run places: {stats['total_places']}\")\n",
    "print(f\" Run places: {stats['total_places']}\")\n",
    "print(f\"\\n Checkpoint system working!\")\n",
    "print(f\" Second run used existing checkpoint to avoid re-processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece73b1",
   "metadata": {},
   "source": [
    "## Test 7: Visualize Collection Results\n",
    "\n",
    "**What this does:** Creates visualizations of collected reviews\n",
    "\n",
    "**Outputs:**\n",
    "- Rating distribution histogram\n",
    "- Reviews per agency bar chart\n",
    "- Review length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae19e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load most recent reviews CSV (check new data architecture first)\n",
    "output_dir = project_root / \"data\" / \"0_processed\" / \"collection\"\n",
    "if not output_dir.exists():\n",
    " output_dir = project_root / \"data\" / \"output\" # Fallback to legacy\n",
    "\n",
    "review_files = sorted(output_dir.glob(\"*reviews*.csv\"), reverse=True)\n",
    "\n",
    "if review_files:\n",
    " latest_file = review_files[0]\n",
    " print(f\" Loading: {latest_file.name}\\n\")\n",
    " \n",
    " reviews_df = pd.read_csv(latest_file)\n",
    " \n",
    " print(f\" REVIEW VISUALIZATIONS\")\n",
    " print(f\"=\"*60)\n",
    " print(f\"Total reviews: {len(reviews_df)}\\n\")\n",
    " \n",
    " # Detect business column (support both old and new)\n",
    " business_col = None\n",
    " if '_business' in reviews_df.columns:\n",
    " business_col = '_business'\n",
    " elif '_bank' in reviews_df.columns:\n",
    " business_col = '_bank'\n",
    " \n",
    " # Set style\n",
    " plt.style.use('seaborn-v0_8-whitegrid')\n",
    " \n",
    " # . Rating distribution\n",
    " if 'rating' in reviews_df.columns:\n",
    " fig, ax = plt.subplots(figsize=(0, 6))\n",
    " rating_counts = reviews_df['rating'].value_counts().sort_index()\n",
    " colors = ['#d678', '#ff7f0e', '#ffbb78', '#98df8a', '#ca0c']\n",
    " rating_counts.plot(kind='bar', ax=ax, color=colors)\n",
    " ax.set_title('Review Rating Distribution', fontsize=, fontweight='bold')\n",
    " ax.set_xlabel('Rating (Stars)', fontsize=)\n",
    " ax.set_ylabel('Number of Reviews', fontsize=)\n",
    " ax.set_xticklabels([f'{int(x)} ' for x in rating_counts.index], rotation=0)\n",
    " ax.grid(axis='y', alpha=0.)\n",
    " \n",
    " # Add value labels on bars\n",
    " for i, v in enumerate(rating_counts):\n",
    " ax.text(i, v + 0., str(v), ha='center', va='bottom')\n",
    " \n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " \n",
    " # . Reviews per business location\n",
    " if business_col and business_col in reviews_df.columns:\n",
    " review_counts = reviews_df.groupby(business_col).size().sort_values(ascending=False).head(0)\n",
    " \n",
    " fig, ax = plt.subplots(figsize=(0, 6))\n",
    " review_counts.plot(kind='barh', ax=ax, color='steelblue')\n",
    " ax.set_title('Reviews per Business Location (Top 0)', fontsize=, fontweight='bold')\n",
    " ax.set_xlabel('Number of Reviews', fontsize=)\n",
    " ax.set_ylabel('Business', fontsize=)\n",
    " ax.grid(axis='x', alpha=0.)\n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " \n",
    " # . Review length distribution\n",
    " if 'text' in reviews_df.columns:\n",
    " reviews_df['text_length'] = reviews_df['text'].fillna('').str.len()\n",
    " \n",
    " fig, ax = plt.subplots(figsize=(0, 6))\n",
    " ax.hist(reviews_df['text_length'], bins=0, color='coral', edgecolor='black', alpha=0.7)\n",
    " ax.set_title('Review Length Distribution', fontsize=, fontweight='bold')\n",
    " ax.set_xlabel('Review Length (characters)', fontsize=)\n",
    " ax.set_ylabel('Frequency', fontsize=)\n",
    " ax.axvline(reviews_df['text_length'].mean(), color='red', linestyle='--', label=f'Mean: {reviews_df[\"text_length\"].mean():.0f}')\n",
    " ax.axvline(reviews_df['text_length'].median(), color='green', linestyle='--', label=f'Median: {reviews_df[\"text_length\"].median():.0f}')\n",
    " ax.legend()\n",
    " ax.grid(axis='y', alpha=0.)\n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " \n",
    "else:\n",
    " print(\" No review files found. Run a test first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2ee05",
   "metadata": {},
   "source": [
    "## Export for Next Step\n",
    "\n",
    "**What this does:** Prepares review data for Step (Classification)\n",
    "\n",
    "**Output:** Clean CSV file ready for classification notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for classification or transform\n",
    "if review_files:\n",
    " reviews_df = pd.read_csv(review_files[0])\n",
    " \n",
    " # Detect business column (support both old and new)\n",
    " business_col = '_business' if '_business' in reviews_df.columns else '_bank'\n",
    " \n",
    " # Prepare for next step (transform then classification)\n",
    " required_columns = ['_place_id', '_city', business_col, 'text', 'rating']\n",
    " available_columns = [col for col in reviews_df.columns if col in required_columns or col.startswith('_')]\n",
    " \n",
    " # Keep only reviews with text\n",
    " if 'text' in reviews_df.columns:\n",
    " df_clean = reviews_df[reviews_df['text'].notna()].copy()\n",
    " else:\n",
    " df_clean = reviews_df.copy()\n",
    " \n",
    " # Save to interim for transform step (new data architecture)\n",
    " next_step_file = project_root / \"data\" / \"0_interim\" / \"collection\" / \"reviews.parquet\"\n",
    " next_step_file.parent.mkdir(parents=True, exist_ok=True)\n",
    " df_clean.to_parquet(next_step_file, index=False)\n",
    " \n",
    " # Also save CSV for backward compatibility\n",
    " next_step_csv = project_root / \"data\" / \"0_interim\" / \"collection\" / \"reviews.csv\"\n",
    " df_clean.to_csv(next_step_csv, index=False)\n",
    " \n",
    " print(f\" EXPORT COMPLETE\")\n",
    " print(f\"=\"*60)\n",
    " print(f\" Parquet: {next_step_file.name}\")\n",
    " print(f\" CSV: {next_step_csv.name}\")\n",
    " print(f\" Records: {len(df_clean)}\")\n",
    " print(f\" Columns: {list(df_clean.columns)}\")\n",
    " \n",
    " if 'text' in df_clean.columns:\n",
    " reviews_with_text = df_clean['text'].notna().sum()\n",
    " print(f\" Reviews with text: {reviews_with_text} ({reviews_with_text/len(df_clean)*00:.f}%)\")\n",
    " \n",
    " # Show business breakdown\n",
    " if business_col in df_clean.columns:\n",
    " business_counts = df_clean[business_col].value_counts()\n",
    " print(f\"\\n Reviews by business:\")\n",
    " for business, count in business_counts.head().items():\n",
    " print(f\" - {business}: {count}\")\n",
    " if len(business_counts) > :\n",
    " print(f\" ... and {len(business_counts) - } more\")\n",
    " \n",
    " print(f\"\\n Data saved to: data/0_interim/collection/\")\n",
    " print(f\"\\n Next steps:\")\n",
    " print(f\" . Transform reviews (normalize + add region)\")\n",
    " print(f\" Run: python -m review_analyzer.main transform\")\n",
    " print(f\" . Classify reviews (extract sentiments/topics)\")\n",
    " print(f\" Open: classify_reviews.ipynb\")\n",
    "else:\n",
    " print(\" No review files found. Run a test first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ac302",
   "metadata": {},
   "source": [
    "## Summary for New Developers\n",
    "\n",
    "**What you learned:**\n",
    "\n",
    ". **ReviewCollector** - The main class for collecting reviews from Google Maps\n",
    ". **Enhanced Place ID Support** - Automatic handling of canonical place IDs, resolution status, and data IDs\n",
    ". **Output modes:**\n",
    " - `csv` - Single CSV file (easiest for analysis)\n",
    " - `json` - Single JSON file (structured data)\n",
    " - `json-per-city` - Separate files per city\n",
    " - `json-per-business` - Separate files per business location\n",
    ". **Checkpoint system** - Automatic save/resume for long operations (prevents data loss)\n",
    ". **Review metadata** - Ratings, dates, authors, full review text\n",
    "6. **New data architecture** - Organized folder structure (00_config, 0_raw, 0_interim, 0_processed, 0_analysis)\n",
    "\n",
    "**Key takeaways:**\n",
    "- System auto-detects canonical place IDs from enhanced discovery output\n",
    "- Start with single business location tests before running full collections\n",
    "- Use checkpoints for large collections to enable safe interruption and resume\n",
    "- Choose output mode based on your analysis needs\n",
    "- Always validate collected data with visualizations\n",
    "- Data is saved to `data/0_interim/collection/` for next pipeline stage\n",
    "\n",
    "**Output mode decision guide:**\n",
    "- `csv` For spreadsheet analysis, simple processing, data exploration\n",
    "- `json` For programmatic access, single file simplicity\n",
    "- `json-per-city` For city-level reports or parallel processing by city\n",
    "- `json-per-business` For business-level reports or competitive analysis\n",
    "\n",
    "**New features (Enhanced Discovery):**\n",
    "- **Canonical Place IDs**: System validates and resolves place IDs to standardized ChIJ format\n",
    "- **Resolution Status**: Track how place IDs were resolved (cache, api_resolved, already_canonical, etc.)\n",
    "- **OSM Integration**: Automatic city alias resolution (e.g., \"Kénitra\" \"Kenitra\")\n",
    "- **Backward Compatibility**: Works with both old (`_bank`) and new (`_business`) data formats\n",
    "\n",
    "**Data Flow (New Architecture):**\n",
    "```\n",
    "discover collect transform classify\n",
    " ↓ ↓ ↓ ↓\n",
    "0_raw/ 0_interim/ 0_interim/ 0_processed/\n",
    "discovery collection transform classification\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    ". You've discovered place_ids (Step : discover_placeids.ipynb)\n",
    ". You've collected reviews (Step : collect_reviews.ipynb - this notebook!)\n",
    ". Transform reviews - normalize fields, add regions (CLI: `python -m review_analyzer.main transform`)\n",
    ". Classify reviews to extract topics and sentiments (Step : classify_reviews.ipynb)\n",
    "\n",
    "**Pipeline Command (Run all steps):**\n",
    "```bash\n",
    "python -m review_analyzer.main pipeline \\\n",
    " --businesses \"Attijariwafa Bank\" \\\n",
    " --cities \"Casablanca\" \\\n",
    " --business-type \"bank\"\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}