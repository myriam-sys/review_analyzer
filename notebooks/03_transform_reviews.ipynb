{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfcd91a",
   "metadata": {},
   "source": [
    "# Step .: Review Transformation - Tutorial\n",
    "\n",
    "**Purpose:** Transform and enrich collected reviews before classification\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to normalize review fields (dates, ratings, text)\n",
    "- How to add geographic regions to reviews\n",
    "- How to build aggregates for analysis\n",
    "- How to handle missing data gracefully\n",
    "\n",
    "**For Junior Developers:**\n",
    "- This step is optional but highly recommended\n",
    "- Transforms raw collected data into analysis-ready format\n",
    "- Adds regional context for geographic analysis\n",
    "- Creates pre-computed aggregates for faster dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3cf5df",
   "metadata": {},
   "source": [
    "## What's New: Transform Pipeline\n",
    "\n",
    "This notebook demonstrates the **new transform step** in the pipeline:\n",
    "\n",
    "### Transform Components:\n",
    ". **Normalize Reviews** (`normalize_reviews.py`):\n",
    " - Parse French relative dates (\"il y a mois\" datetime)\n",
    " - Clean and validate ratings (clip to - range)\n",
    " - Normalize city names (remove accents, lowercase)\n",
    " - Extract month for temporal analysis\n",
    "\n",
    ". **Add Regions** (`geocode.py`):\n",
    " - Use GeoJSON polygons to assign Moroccan regions\n",
    " - Fallback to city-name mapping when coordinates unavailable\n",
    " - Validate coordinate bounds (Morocco only)\n",
    "\n",
    ". **Build Aggregates** (`aggregates.py`):\n",
    " - Pre-compute statistics by business, city, region, month\n",
    " - Count reviews, average ratings, sentiment distribution\n",
    " - Ready for dashboards and reports\n",
    "\n",
    "### Pipeline Flow:\n",
    "```\n",
    "discover collect TRANSFORM classify\n",
    " ↓ ↓ ↓ ↓\n",
    "0_raw/ 0_interim/ 0_interim/ 0_processed/\n",
    "discovery collection transform classification\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316479d",
   "metadata": {},
   "source": [
    "## Data Architecture Overview\n",
    "\n",
    "The pipeline uses an organized folder structure:\n",
    "\n",
    "```\n",
    "data/\n",
    " 00_config/ # Static configurations\n",
    " cities/ # regions.geojson for geocoding\n",
    " 0_raw/ # Immutable source data\n",
    " 0_interim/ # Recomputable cache\n",
    " collection/ # Collected reviews (input for transform)\n",
    " transform/ # Normalized + enriched reviews\n",
    " 0_processed/ # Final outputs\n",
    " transform/ # Aggregates for analysis\n",
    " 0_analysis/ # Reports, figures, dashboards\n",
    "```\n",
    "\n",
    "**This notebook processes:**\n",
    "- Input: `data/0_interim/collection/reviews.parquet` (from collect step)\n",
    "- Output: `data/0_interim/transform/reviews_normalized.parquet`\n",
    "- Aggregates: `data/0_processed/transform/aggregates_*.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b6e3a",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94128e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import transform modules\n",
    "from review_analyzer.transformers.normalize_reviews import normalize_reviews_df\n",
    "from review_analyzer.transformers.geocode import add_region, add_region_by_city, CITY_REGION_MAPPING\n",
    "from review_analyzer.transformers.aggregates import build_aggregates\n",
    "from review_analyzer import config\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"\\nTransform components loaded:\")\n",
    "print(f\" - normalize_reviews_df (date parsing, field cleaning)\")\n",
    "print(f\" - add_region (GeoJSON-based geocoding)\")\n",
    "print(f\" - add_region_by_city (city-name fallback)\")\n",
    "print(f\" - build_aggregates (pre-computed statistics)\")\n",
    "print(f\"\\nCity-to-region mapping: {len(CITY_REGION_MAPPING)} cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a043415",
   "metadata": {},
   "source": [
    "## Test : Load Raw Reviews\n",
    "\n",
    "**What this does:** Loads reviews from the collection step\n",
    "\n",
    "**Expected input:** Reviews with raw dates, text, ratings from Step \n",
    "\n",
    "**Expected columns:** `text`, `date`, `rating`, `lat`, `lng`, `city`, `business_id`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews from collection step\n",
    "# Try multiple locations in order of preference\n",
    "input_file = None\n",
    "\n",
    "candidates = [\n",
    " project_root / \"data\" / \"0_interim\" / \"collection\" / \"reviews.parquet\",\n",
    " project_root / \"data\" / \"0_interim\" / \"collection\" / \"reviews.csv\",\n",
    " project_root / \"data\" / \"0_interim\" / \"collection\" / \"bank_reviews.csv\",\n",
    " project_root / \"data\" / \"output\" / \"reviews_for_classification.csv\", # Legacy\n",
    "]\n",
    "\n",
    "for candidate in candidates:\n",
    " if candidate.exists():\n",
    " input_file = candidate\n",
    " break\n",
    "\n",
    "if input_file and input_file.exists():\n",
    " # Load based on file type\n",
    " if input_file.suffix == '.parquet':\n",
    " reviews_df = pd.read_parquet(input_file)\n",
    " else:\n",
    " # Try multiple encodings for CSV files (French text may use Latin-)\n",
    " for encoding in ['utf-8', 'latin-', 'cp', 'utf-8-sig']:\n",
    " try:\n",
    " reviews_df = pd.read_csv(input_file, encoding=encoding)\n",
    " print(f\" Loaded with encoding: {encoding}\")\n",
    " break\n",
    " except UnicodeDecodeError:\n",
    " continue\n",
    " else:\n",
    " # Fallback: read with errors='replace' to handle any encoding\n",
    " reviews_df = pd.read_csv(input_file, encoding='utf-8', errors='replace')\n",
    " print(f\" Loaded with fallback encoding (some characters may be replaced)\")\n",
    " \n",
    " print(f\" RAW REVIEWS LOADED\")\n",
    " print(f\"=\"*80)\n",
    " print(f\" Source: {input_file.relative_to(project_root)}\")\n",
    " print(f\" Total reviews: {len(reviews_df)}\")\n",
    " print(f\" Columns: {list(reviews_df.columns)}\")\n",
    " \n",
    " # Show data types\n",
    " print(f\"\\n Data types:\")\n",
    " for col in ['date', 'rating', 'lat', 'lng', 'city', 'text']:\n",
    " if col in reviews_df.columns:\n",
    " print(f\" {col}: {reviews_df[col].dtype}\")\n",
    " \n",
    " # Show sample\n",
    " print(f\"\\n Sample raw data:\")\n",
    " sample_cols = [c for c in ['date', 'rating', 'city', 'text'] if c in reviews_df.columns]\n",
    " display(reviews_df[sample_cols].head())\n",
    " \n",
    " # Check for issues\n",
    " print(f\"\\n Data quality checks:\")\n",
    " if 'date' in reviews_df.columns:\n",
    " french_dates = reviews_df['date'].astype(str).str.contains('il y a', na=False).sum()\n",
    " print(f\" French relative dates: {french_dates}/{len(reviews_df)} (need parsing)\")\n",
    " \n",
    " if 'rating' in reviews_df.columns:\n",
    " print(f\" Rating type: {reviews_df['rating'].dtype} (should be numeric)\")\n",
    " print(f\" Rating range: {reviews_df['rating'].min()} - {reviews_df['rating'].max()}\")\n",
    " \n",
    " if 'city' in reviews_df.columns:\n",
    " cities_with_accents = reviews_df['city'].dropna().str.contains('[éèêëàâäùûüôö]', regex=True, na=False).sum()\n",
    " print(f\" Cities with accents: {cities_with_accents} (need normalization)\")\n",
    " \n",
    " print(f\"\\n This data needs transformation!\")\n",
    " \n",
    "else:\n",
    " print(f\" No reviews file found!\")\n",
    " print(f\"\\n Tried locations:\")\n",
    " for candidate in candidates:\n",
    " print(f\" - {candidate.relative_to(project_root)}\")\n",
    " print(f\"\\n Please run collect_reviews.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92370d2f",
   "metadata": {},
   "source": [
    "## Test : Normalize Review Fields\n",
    "\n",
    "**What this does:** Cleans and normalizes review data\n",
    "\n",
    "**Transformations:**\n",
    "- Parse French relative dates (\"il y a mois\" datetime)\n",
    "- Convert ratings to Int6 and clip to - range\n",
    "- Normalize city names (remove accents, lowercase)\n",
    "- Create `created_at` and `month` columns for temporal analysis\n",
    "- Clean text fields (strip whitespace)\n",
    "\n",
    "**Expected output:** DataFrame with datetime, Int6 ratings, normalized fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize reviews\n",
    "if input_file and input_file.exists():\n",
    " print(f\" NORMALIZING REVIEW FIELDS\")\n",
    " print(f\"=\"*80)\n",
    " print(f\" Processing {len(reviews_df)} reviews...\\n\")\n",
    " \n",
    " # Check date column before normalization\n",
    " date_col = 'review_date' if 'review_date' in reviews_df.columns else 'date'\n",
    " if date_col in reviews_df.columns:\n",
    " print(f\" Raw date column: '{date_col}'\")\n",
    " print(f\" Sample values (repr to see hidden characters):\")\n",
    " for i, val in enumerate(reviews_df[date_col].head()):\n",
    " print(f\" [{i}] {repr(val)}\")\n",
    " \n",
    " # Apply normalization\n",
    " normalized_df = normalize_reviews_df(reviews_df.copy())\n",
    " \n",
    " print(f\"\\n NORMALIZATION COMPLETE\")\n",
    " print(f\"=\"*80)\n",
    " \n",
    " # Compare before/after\n",
    " print(f\"\\n New columns added:\")\n",
    " new_cols = set(normalized_df.columns) - set(reviews_df.columns)\n",
    " if new_cols:\n",
    " for col in sorted(new_cols):\n",
    " print(f\" + {col}: {normalized_df[col].dtype}\")\n",
    " else:\n",
    " print(f\" (none)\")\n",
    " \n",
    " # Show transformations\n",
    " print(f\"\\n Field transformations:\")\n",
    " \n",
    " if 'created_at' in normalized_df.columns:\n",
    " parsed_dates = normalized_df['created_at'].notna().sum()\n",
    " unparsed_dates = normalized_df['created_at'].isna().sum()\n",
    " print(f\" Dates parsed: {parsed_dates}/{len(normalized_df)} ({parsed_dates/len(normalized_df)*00:.f}%)\")\n",
    " print(f\" Type: {normalized_df['created_at'].dtype}\")\n",
    " if parsed_dates > 0:\n",
    " print(f\" Range: {normalized_df['created_at'].min()} to {normalized_df['created_at'].max()}\")\n",
    " if unparsed_dates > 0:\n",
    " print(f\" Unparsed: {unparsed_dates} dates could not be parsed\")\n",
    " # Show examples of unparsed dates\n",
    " unparsed_examples = reviews_df.loc[normalized_df['created_at'].isna(), date_col].head().tolist()\n",
    " if unparsed_examples:\n",
    " print(f\" Examples: {unparsed_examples}\")\n",
    " else:\n",
    " print(f\" created_at NOT created - check date column format!\")\n",
    " if date_col in reviews_df.columns:\n",
    " print(f\" Raw date samples: {reviews_df[date_col].head().tolist()}\")\n",
    " \n",
    " if 'rating' in normalized_df.columns:\n",
    " print(f\" Ratings normalized: {normalized_df['rating'].dtype}\")\n",
    " print(f\" Range: {normalized_df['rating'].min()} - {normalized_df['rating'].max()} (clipped to -)\")\n",
    " \n",
    " if 'city_normalized' in normalized_df.columns:\n",
    " unique_cities = normalized_df['city_normalized'].nunique()\n",
    " print(f\" Cities normalized: {unique_cities} unique cities\")\n",
    " print(f\" Sample: {normalized_df['city_normalized'].dropna().unique()[:].tolist()}\")\n",
    " \n",
    " if 'month' in normalized_df.columns:\n",
    " unique_months = normalized_df['month'].nunique()\n",
    " print(f\" Months extracted: {unique_months} unique months\")\n",
    " print(f\" Range: {normalized_df['month'].min()} to {normalized_df['month'].max()}\")\n",
    " \n",
    " # Show sample\n",
    " print(f\"\\n Sample normalized data:\")\n",
    " sample_cols = [c for c in ['created_at', 'month', 'rating', 'city_normalized'] if c in normalized_df.columns]\n",
    " if sample_cols:\n",
    " display(normalized_df[sample_cols].head())\n",
    " \n",
    "else:\n",
    " print(\" No data loaded. Run Test first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711ef475",
   "metadata": {},
   "source": [
    "## Test : Add Regions (GeoJSON-based)\n",
    "\n",
    "**What this does:** Assigns Moroccan regions using coordinates and GeoJSON polygons\n",
    "\n",
    "**Method:**\n",
    ". Load `regions.geojson` with Morocco's regions\n",
    ". Use lat/lng to find which polygon contains each review\n",
    ". Validate coordinates are within Morocco bounds\n",
    "\n",
    "**Regions:** administrative regions of Morocco\n",
    "\n",
    "**Expected output:** DataFrame with `region` column added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add regions using GeoJSON\n",
    "if 'normalized_df' in locals():\n",
    " print(f\" ADDING GEOGRAPHIC REGIONS\")\n",
    " print(f\"=\"*80)\n",
    " \n",
    " # Check for regions.geojson\n",
    " regions_file = config.REGIONS_FILE\n",
    " \n",
    " if regions_file.exists():\n",
    " print(f\" Using: {regions_file.relative_to(project_root)}\\n\")\n",
    " \n",
    " # Apply geocoding\n",
    " geocoded_df = add_region(\n",
    " df=normalized_df.copy(),\n",
    " regions_path=regions_file\n",
    " )\n",
    " \n",
    " print(f\"\\n GEOCODING COMPLETE\")\n",
    " print(f\"=\"*80)\n",
    " \n",
    " # Statistics\n",
    " if 'region' in geocoded_df.columns:\n",
    " regions_assigned = geocoded_df['region'].notna().sum()\n",
    " regions_missing = geocoded_df['region'].isna().sum()\n",
    " \n",
    " print(f\"\\n Results:\")\n",
    " print(f\" Regions assigned: {regions_assigned}/{len(geocoded_df)} ({regions_assigned/len(geocoded_df)*00:.f}%)\")\n",
    " print(f\" Missing regions: {regions_missing}/{len(geocoded_df)} ({regions_missing/len(geocoded_df)*00:.f}%)\")\n",
    " \n",
    " # Region distribution\n",
    " print(f\"\\n Region distribution:\")\n",
    " region_counts = geocoded_df['region'].value_counts()\n",
    " for region, count in region_counts.items():\n",
    " percentage = (count / len(geocoded_df)) * 00\n",
    " bar = '' * int(percentage / )\n",
    " print(f\" {region:0} {count:} {bar} {percentage:.f}%\")\n",
    " \n",
    " # Show sample\n",
    " print(f\"\\n Sample with regions:\")\n",
    " sample_cols = [c for c in ['city', 'lat', 'lng', 'region'] if c in geocoded_df.columns]\n",
    " display(geocoded_df[sample_cols].head())\n",
    " \n",
    " else:\n",
    " print(f\" regions.geojson not found at: {regions_file}\")\n",
    " print(f\" Using city-name fallback instead...\\n\")\n",
    " \n",
    " # Fallback to city-name mapping\n",
    " geocoded_df = add_region_by_city(normalized_df.copy())\n",
    " \n",
    " print(f\"\\n GEOCODING COMPLETE (City-name fallback)\")\n",
    " print(f\"=\"*80)\n",
    " \n",
    " if 'region' in geocoded_df.columns:\n",
    " regions_assigned = geocoded_df['region'].notna().sum()\n",
    " print(f\"\\n Results:\")\n",
    " print(f\" Regions assigned: {regions_assigned}/{len(geocoded_df)}\")\n",
    " print(f\"\\n Region distribution:\")\n",
    " region_counts = geocoded_df['region'].value_counts()\n",
    " for region, count in region_counts.head(0).items():\n",
    " print(f\" {region:0} {count:}\")\n",
    " \n",
    " print(f\"\\n For better accuracy, add regions.geojson to data/00_config/cities/\")\n",
    " \n",
    "else:\n",
    " print(\" No normalized data. Run Test first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49e562",
   "metadata": {},
   "source": [
    "## Test : Visualize Transformations\n",
    "\n",
    "**What this does:** Creates visualizations showing data quality improvements\n",
    "\n",
    "**Visualizations:**\n",
    "- Date distribution (before/after parsing)\n",
    "- Region coverage map\n",
    "- Rating distribution\n",
    "- Temporal trends (reviews per month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformations\n",
    "if 'geocoded_df' in locals():\n",
    " print(f\" TRANSFORMATION VISUALIZATIONS\")\n",
    " print(f\"=\"*80)\n",
    " print(f\"\\nCreating charts...\\n\")\n",
    " \n",
    " # Set style\n",
    " plt.style.use('seaborn-v0_8-whitegrid')\n",
    " \n",
    " # . Date parsing success\n",
    " if 'created_at' in geocoded_df.columns:\n",
    " fig, ax = plt.subplots(figsize=(0, 6))\n",
    " \n",
    " parsed = geocoded_df['created_at'].notna().sum()\n",
    " unparsed = geocoded_df['created_at'].isna().sum()\n",
    " \n",
    " ax.bar(['Parsed', 'Unparsed'], [parsed, unparsed], color=['#ecc7', '#e7cc'])\n",
    " ax.set_title('Date Parsing Results', fontsize=, fontweight='bold')\n",
    " ax.set_ylabel('Number of Reviews', fontsize=)\n",
    " ax.grid(axis='y', alpha=0.)\n",
    " \n",
    " # Add percentage labels\n",
    " total = len(geocoded_df)\n",
    " ax.text(0, parsed + 0, f'{parsed/total*00:.f}%', ha='center', fontsize=)\n",
    " ax.text(, unparsed + 0, f'{unparsed/total*00:.f}%', ha='center', fontsize=)\n",
    " \n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " \n",
    " # . Region coverage\n",
    " if 'region' in geocoded_df.columns:\n",
    " fig, ax = plt.subplots(figsize=(, 6))\n",
    " \n",
    " region_counts = geocoded_df['region'].value_counts().sort_values(ascending=True)\n",
    " region_counts.plot(kind='barh', ax=ax, color='steelblue')\n",
    " \n",
    " ax.set_title('Reviews by Region', fontsize=, fontweight='bold')\n",
    " ax.set_xlabel('Number of Reviews', fontsize=)\n",
    " ax.set_ylabel('Region', fontsize=)\n",
    " ax.grid(axis='x', alpha=0.)\n",
    " \n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " \n",
    " # . Temporal trends\n",
    " if 'month' in geocoded_df.columns:\n",
    " fig, ax = plt.subplots(figsize=(, 6))\n",
    " \n",
    " month_counts = geocoded_df['month'].value_counts().sort_index()\n",
    " month_counts.plot(kind='line', ax=ax, marker='o', color='coral', linewidth=)\n",
    " \n",
    " ax.set_title('Reviews Over Time', fontsize=, fontweight='bold')\n",
    " ax.set_xlabel('Month', fontsize=)\n",
    " ax.set_ylabel('Number of Reviews', fontsize=)\n",
    " ax.grid(axis='both', alpha=0.)\n",
    " \n",
    " plt.xticks(rotation=, ha='right')\n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " \n",
    " # . Rating distribution\n",
    " if 'rating' in geocoded_df.columns:\n",
    " fig, ax = plt.subplots(figsize=(0, 6))\n",
    " \n",
    " rating_counts = geocoded_df['rating'].value_counts().sort_index()\n",
    " colors = ['#d678', '#ff7f0e', '#ffbb78', '#98df8a', '#ca0c']\n",
    " rating_counts.plot(kind='bar', ax=ax, color=colors)\n",
    " \n",
    " ax.set_title('Rating Distribution (After Normalization)', fontsize=, fontweight='bold')\n",
    " ax.set_xlabel('Rating (Stars)', fontsize=)\n",
    " ax.set_ylabel('Number of Reviews', fontsize=)\n",
    " ax.set_xticklabels([f'{int(x)} ' for x in rating_counts.index], rotation=0)\n",
    " ax.grid(axis='y', alpha=0.)\n",
    " \n",
    " plt.tight_layout()\n",
    " plt.show()\n",
    " \n",
    "else:\n",
    " print(\" No geocoded data. Run Test first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195577c",
   "metadata": {},
   "source": [
    "## Test : Save Transformed Data\n",
    "\n",
    "**What this does:** Saves transformed reviews for the classification step\n",
    "\n",
    "**Output location:** `data/0_interim/transform/reviews_normalized.parquet`\n",
    "\n",
    "**Format:** Parquet (efficient, preserves types)\n",
    "\n",
    "**This data is now ready for classification!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save transformed data\n",
    "if 'geocoded_df' in locals():\n",
    " # Output paths\n",
    " output_parquet = project_root / \"data\" / \"0_interim\" / \"transform\" / \"reviews_normalized.parquet\"\n",
    " output_csv = project_root / \"data\" / \"0_interim\" / \"transform\" / \"reviews_normalized.csv\"\n",
    " \n",
    " # Create directories\n",
    " output_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    " \n",
    " # Save both formats\n",
    " geocoded_df.to_parquet(output_parquet, index=False)\n",
    " geocoded_df.to_csv(output_csv, index=False)\n",
    " \n",
    " print(f\" TRANSFORMED DATA SAVED\")\n",
    " print(f\"=\"*80)\n",
    " print(f\" Parquet: {output_parquet.relative_to(project_root)}\")\n",
    " print(f\" CSV: {output_csv.relative_to(project_root)}\")\n",
    " print(f\" Records: {len(geocoded_df)}\")\n",
    " print(f\" Columns: {len(geocoded_df.columns)}\")\n",
    " \n",
    " # Summary of transformations\n",
    " print(f\"\\n TRANSFORMATION SUMMARY:\")\n",
    " \n",
    " if 'created_at' in geocoded_df.columns:\n",
    " parsed = geocoded_df['created_at'].notna().sum()\n",
    " print(f\"\\n Dates:\")\n",
    " print(f\" Parsed: {parsed}/{len(geocoded_df)} ({parsed/len(geocoded_df)*00:.f}%)\")\n",
    " \n",
    " if 'rating' in geocoded_df.columns:\n",
    " print(f\"\\n Ratings:\")\n",
    " print(f\" Type: {geocoded_df['rating'].dtype} (Int6)\")\n",
    " print(f\" Range: {geocoded_df['rating'].min()} - {geocoded_df['rating'].max()}\")\n",
    " print(f\" Average: {geocoded_df['rating'].mean():.f}\")\n",
    " \n",
    " if 'region' in geocoded_df.columns:\n",
    " regions_assigned = geocoded_df['region'].notna().sum()\n",
    " print(f\"\\n Regions:\")\n",
    " print(f\" Assigned: {regions_assigned}/{len(geocoded_df)} ({regions_assigned/len(geocoded_df)*00:.f}%)\")\n",
    " print(f\" Unique regions: {geocoded_df['region'].nunique()}\")\n",
    " \n",
    " if 'city_normalized' in geocoded_df.columns:\n",
    " print(f\"\\n Cities:\")\n",
    " print(f\" Normalized: {geocoded_df['city_normalized'].nunique()} unique cities\")\n",
    " \n",
    " print(f\"\\n Data saved to: data/0_interim/transform/\")\n",
    " print(f\"\\n Next: Classify reviews (classify_reviews.ipynb)\")\n",
    " print(f\" The classification notebook will automatically load this transformed data!\")\n",
    " \n",
    "else:\n",
    " print(\" No data to save. Run previous tests first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b066b2a",
   "metadata": {},
   "source": [
    "## Test 6: Build Aggregates (Optional)\n",
    "\n",
    "**What this does:** Pre-computes statistics for dashboards and reports\n",
    "\n",
    "**Aggregation levels:**\n",
    "- By business\n",
    "- By city\n",
    "- By region\n",
    "- By month\n",
    "\n",
    "**Metrics computed:**\n",
    "- Review count\n",
    "- Average rating\n",
    "- Sentiment distribution (if classified)\n",
    "- Category counts (if classified)\n",
    "\n",
    "**Note:** This step is most useful after classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build aggregates (optional - more useful after classification)\n",
    "if 'geocoded_df' in locals():\n",
    " print(f\" BUILDING AGGREGATES\")\n",
    " print(f\"=\"*80)\n",
    " print(f\" Note: More metrics available after classification!\\n\")\n",
    " \n",
    " # Output directory\n",
    " output_dir = project_root / \"data\" / \"0_processed\" / \"transform\"\n",
    " output_dir.mkdir(parents=True, exist_ok=True)\n",
    " \n",
    " # Build aggregates\n",
    " try:\n",
    " aggregate_files = build_aggregates(\n",
    " df_labeled=geocoded_df,\n",
    " output_dir=output_dir,\n",
    " date_suffix=True\n",
    " )\n",
    " \n",
    " print(f\"\\n AGGREGATES COMPLETE\")\n",
    " print(f\"=\"*80)\n",
    " print(f\"\\n Files created:\")\n",
    " for key, path in aggregate_files.items():\n",
    " rel_path = Path(path).relative_to(project_root)\n",
    " print(f\" {key}: {rel_path}\")\n",
    " \n",
    " # Show sample from one aggregate\n",
    " if 'by_business.parquet' in aggregate_files:\n",
    " agg_df = pd.read_parquet(aggregate_files['by_business.parquet'])\n",
    " print(f\"\\n Sample: Reviews by Business\")\n",
    " display(agg_df.head())\n",
    " \n",
    " print(f\"\\n These aggregates will be more informative after classification!\")\n",
    " print(f\" Run classify_reviews.ipynb, then re-run this step.\")\n",
    " \n",
    " except Exception as e:\n",
    " print(f\"\\n Error building aggregates: {e}\")\n",
    " print(f\" This is normal if data hasn't been classified yet.\")\n",
    " print(f\" Aggregates work best with sentiment and category data.\")\n",
    " \n",
    "else:\n",
    " print(\" No data available. Run previous tests first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc3345",
   "metadata": {},
   "source": [
    "## Summary for New Developers\n",
    "\n",
    "**What you learned:**\n",
    "\n",
    ". **Transform Pipeline** - The middle step between collection and classification\n",
    ". **Normalization** - Converting raw data into clean, typed, analysis-ready format\n",
    ". **Geocoding** - Adding regional context using GeoJSON polygons or city names\n",
    ". **Aggregation** - Pre-computing statistics for faster dashboards\n",
    ". **Data Flow** - How data moves through the pipeline stages\n",
    "\n",
    "**Key takeaways:**\n",
    "- Transform step is **optional but highly recommended**\n",
    "- Normalization **improves data quality** (dates, ratings, text)\n",
    "- Regions enable **geographic analysis** (heatmaps, regional comparisons)\n",
    "- Aggregates **speed up dashboards** (pre-computed metrics)\n",
    "- Parquet format **preserves types** and is more efficient than CSV\n",
    "- Fallback mechanisms ensure **graceful degradation** (GeoJSON city names)\n",
    "\n",
    "**Transform Components:**\n",
    "\n",
    ". **normalize_reviews_df()** - Main normalization function\n",
    " - Parses French relative dates (\"il y a mois\" datetime)\n",
    " - Converts ratings to Int6, clips to - range\n",
    " - Normalizes city names (removes accents, lowercase)\n",
    " - Extracts month for temporal analysis\n",
    " - Cleans text fields (strips whitespace)\n",
    "\n",
    ". **add_region()** - GeoJSON-based geocoding\n",
    " - Loads regions.geojson with Morocco's regions\n",
    " - Uses shapely to check if coordinates fall within polygons\n",
    " - Validates coordinates are within Morocco bounds\n",
    " - Falls back to city-name mapping if GeoJSON unavailable\n",
    "\n",
    ". **add_region_by_city()** - City-name fallback\n",
    " - Uses CITY_REGION_MAPPING (0+ cities regions)\n",
    " - Case-insensitive matching\n",
    " - Works when coordinates are unavailable\n",
    "\n",
    ". **build_aggregates()** - Pre-computed statistics\n",
    " - Multiple aggregation levels (business, city, region, month)\n",
    " - Smart column detection (works with/without classification)\n",
    " - Saves separate CSV for each level\n",
    " - More useful after classification (includes sentiment/categories)\n",
    "\n",
    "**Data Flow (Complete Pipeline):**\n",
    "```\n",
    "discover collect TRANSFORM classify analyze\n",
    " ↓ ↓ ↓ ↓ ↓\n",
    "0_raw/ 0_interim/ 0_interim/ 0_processed/ 0_analysis/\n",
    "discovery collection transform classification reports\n",
    "```\n",
    "\n",
    "**This notebook processes:**\n",
    "- **Input**: `data/0_interim/collection/reviews.parquet` (from collect step)\n",
    "- **Output**: `data/0_interim/transform/reviews_normalized.parquet`\n",
    "- **Aggregates**: `data/0_processed/transform/aggregates_*.csv`\n",
    "\n",
    "**Next steps:**\n",
    ". You've discovered place_ids (Step : discover_placeids.ipynb)\n",
    ". You've collected reviews (Step : collect_reviews.ipynb)\n",
    ". You've transformed reviews (Step .: transform_reviews.ipynb - this notebook!)\n",
    ". Classify reviews to extract sentiments/topics (Step : classify_reviews.ipynb)\n",
    ". Analyze results, create reports, build dashboards!\n",
    "\n",
    "**Pipeline Command (Run all steps):**\n",
    "```bash\n",
    "python -m review_analyzer.main pipeline \\\n",
    " --businesses \"Attijariwafa Bank\" \\\n",
    " --cities \"Casablanca\" \\\n",
    " --business-type \"bank\"\n",
    "```\n",
    "\n",
    "**Or step by step:**\n",
    "```bash\n",
    "# . Discover\n",
    "python -m review_analyzer.main discover --businesses \"Bank\" --cities \"City\"\n",
    "\n",
    "# . Collect\n",
    "python -m review_analyzer.main collect --input agencies.csv --mode csv\n",
    "\n",
    "# . Transform (this step!)\n",
    "python -m review_analyzer.main transform --regions regions.geojson\n",
    "\n",
    "# . Classify\n",
    "python -m review_analyzer.main classify --input reviews.csv --wide-format\n",
    "```\n",
    "\n",
    "**Troubleshooting:**\n",
    "- No input data? Run collect_reviews.ipynb first\n",
    "- Missing regions? Add regions.geojson to data/00_config/cities/ or use city-name fallback\n",
    "- Aggregate errors? Normal if data not classified yet - run classify_reviews.ipynb first\n",
    "- Date parsing issues? French relative dates are auto-detected (\"il y a X mois/jours/ans\")\n",
    "\n",
    "Happy transforming! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
